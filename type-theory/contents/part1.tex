\section{Introduction}

Dependent type theory is a formal system to organize all mathematical objects,
structure, and knowledge. There are some important different between type theory, set theory and the interpretation of types as sets has significant limitation.

\begin{remark}{(Differences Between Type and Set Theory)}
    There are various differences that we want to discuss between 2 foundations:
    \begin{enumerate}
        \item \textbf{(Every element comes with its type)}, where $a:A$ is the judgement that $a$ an element of type $A$. 
        \item \textbf{(Type Theory is its own formal system)}, while set theory is axiomatized in the formal system of first order logic. Types and their element are constructed by following the formal system rules, and only from previous constructed type
        \begin{enumerate}
            \item Thus, expression $a:A$ is therefore not considered to be proposition (something which one can assert), but a judgement (an assessment that is part of the construction of the element $a:A$). 
        \end{enumerate}
        \item \textbf{(Stronger focus on equality)} of element governed by the identity type, while classical set theory, where equality is a decidable proposition of first order logic. 
        \begin{enumerate}
            \item For example, type $x=_Ay$ of identification of $2$ element $x,y:A$ is itself a type.
            \item We need to know (1) how to construct an element of the type and (2) how to show that $2$ element of the types are equal, in order to understand a type fully. 
        \end{enumerate}
    \end{enumerate}
\end{remark}

\begin{remark}{(Building Dependent Type Theory)}
    To build it, we start with giving structural rule, which express the general theory of type dependency (no ambient deductive system of first order logic in type theory). 
    \begin{itemize}
        \item \textbf{(Deductive System + Operations):} It has its own deductive system, and the structural rules are the heart of this system. The operation that are governed by the structural rules are substitution and weaken operation
        \item \textbf{(Forming Types):} The most fundamental class of types are dependency function type or $\Pi$-types, together with type of natural number (type-theoretic version of induction principle). The induction principle can be used to: (1) construct the operations (2) prove properties about those operations.
        \item \textbf{(Identity Type):} The identity type $x=_Ay$ is an example of a dependent type and is inductively generated by the reflexive element of $\operatorname{refl}_x:x=_Ax$ (beware that it can have many different element).
        \item \textbf{(Universe):} Type families that are closed under the operations of type theory: $\Pi$-types, $\Sigma$-types, identity types and so on. They can be used to define type families over inductive types via the induction properties. 
        \begin{itemize}
            \item For example, defining ordering relation $\le$ and $<$ on the natural number, or showing that $\succnat$ is injective, and $0_\mathbb{Z}$ isn't successor from Peano axiom.
        \end{itemize}
    \end{itemize}
\end{remark}

\section{Dependent Type Theory}

Usually the goal is to construct an element of certain type. For example, an element is a function if the type of the constructed element is a function type; proof of property if the type is a proposition; identification if the type is an identity type, etc. 

\textit{A type is just a collection of mathematical object and constructing element of a type is that every mathematics task or challenge}, and the system of inference rules (type theory) offer a principled way to do it.

\subsection{Judgement and Context in Type Theory}

\begin{definition}{\textbf{(Inference Rules)}}
    Construction consists of sequence of deductive steps, using finitely many premises (judgement), such steps can be represented by inference rule written as:
    \begin{equation*}
        \frac{\mathcal{H}_1\qquad\mathcal{H}_2\quad\cdots\quad\mathcal{H}_n}{\mathcal{C}}
    \end{equation*}
    We ended up getting the single judgement from the rule.
\end{definition}

\begin{definition}{\textbf{(Judgement)}}
    There are 4 types of judgement in the type theory:
    \begin{itemize}
        \item $A$ is a (well-formed) type in context $\Gamma$, expressed as: $\Gamma\vdash A \type $
        \item $A$ and $B$ are judgmentally equal types in context $\Gamma$, expressed as $\Gamma\vdash A\equiv B\type $
        \item $a$ is an element of type $A$ in context $\Gamma$, expressed as $\Gamma\vdash a:A$
        \item $a$ and $b$ are judgmentally equal elements of type $A$ in context $\Gamma$, expressed as $\Gamma\vdash a \ \equiv \ b:A$
    \end{itemize}

\end{definition}

The role of a context is to declare what hypothetical elements (commonly called variables) are assumed, along with their types.

\begin{definition}{\textbf{(Context)}}
    It is an expression of the form:
    \begin{equation*}
        x_1:A_1 \qquad x_2:A_2(x_1) \qquad \cdots \qquad x_n:A_n(x_1,\dots,x_{n-1})
    \end{equation*}
    satisfying the condition that for each $1\le k\le n$, we can derive them using the inference rules of type theory i.e:
    \begin{equation*}
        x_1:A_1, x_2:A_2(x_1) , \cdots, x_{k-1}:A_{k-1}(x_1, \dots, x_{k-2})\vdash A_k(x_1,\dots,x_{k-1}) \type 
    \end{equation*}
    There is a context of length $0$, the empty context, which declare no variable, and satisfies this condition. A list of variable declaration $x_1:A_1$ of length $1$ is context iff $A_1$ is a type in the empty context.
\end{definition}

In dependent type theory, all judgement are context dependent, and the types of the variable in a context may depend on any previous declared variables.

\begin{definition}{\textbf{(Type Family)}}
    Consider a type $A$ in context $\Gamma$, a family of types over $A$ in context $\Gamma$ is a type $B(x)$ in context $\Gamma,x:A$ where $\Gamma,x:A\vdash B(x)\type $. Alternatively, we can say that $B(x)$ is a type indexed by $x:A$ in context $\Gamma$
\end{definition}


The basic example of a type family occurs when we introduce identity types, introduced as, which asserts that given an element $a:A$ in context $\Gamma$, we may form the type $a=x$ in context $\Gamma,x:A$:

\begin{equation*}
    \frac{\Gamma\vdash a:A}{\Gamma,x:A\vdash a=x\type }
\end{equation*}

\begin{definition}{\textbf{(Section)}}
    Consider a type family $B$ over $A$ in context $\Gamma$. A section of the family $B$ over $A$ in context $\Gamma$ is an element of type $B(x)$ in context $\Gamma,x:A$. In a judgement: $\Gamma,x:A\vdash b(x):B(x)$, we say that $b$ is a section of family $B$ over $A$. Alternatively, we say that $b(x)$ is an element of type $B(x)$ indexed by $x:A$ in context $\Gamma$.
\end{definition}

\subsection{Inference Rule}

We will present the system of inference rules, known as structural rules of type theory. There are $5$ sets of inference rule: (1) Judgemental equality is an equivalent relation (2) Variable conversion rules (3) Substitution rules (4) Weakening rules (5) Generic element.


\begin{definition}{\textbf{(Judgemental Equality as Equivalent Relation)}}
    These rules assert that the relation are reflexive, symmetric and transitive:
    \begin{equation*}
    \begin{aligned}
        &\frac{\Gamma\vdash A\type }{\Gamma\vdash A\equiv A\type } \qquad \frac{\Gamma\vdash A'\equiv A\type }{\Gamma\vdash A\equiv A'\type } \qquad \frac{\Gamma\vdash A\equiv A'\type \qquad \Gamma\vdash A'\equiv A''\type }{\Gamma\vdash A\equiv A''\type } \\
        &\frac{\Gamma\vdash a:A}{\Gamma\vdash a\equiv a:A} \qquad \frac{\Gamma\vdash a\equiv a':A}{\Gamma\vdash a'\equiv a:A} \qquad \frac{\Gamma\vdash a\equiv a':A\qquad \Gamma\vdash a'\equiv a'':A}{\Gamma\vdash a\equiv a'':A}
    \end{aligned}
    \end{equation*}
\end{definition}

\begin{definition}{\textbf{(Variable Conversion Rule)}}
    The rules assert that we can convert the type of a variable to judgmentally equal types:
    \begin{equation*}
        \frac{\Gamma\vdash A\equiv A'\type \qquad\Gamma, x:A,\Delta\vdash\mathcal{J}}{\Gamma, x:A',\Delta\vdash\mathcal{J}}
    \end{equation*}
    where $\mathcal{J}$ is generic judgment thesis as representing all kinds of judgement. For example, it can be $B(x)\type $. We do this so that we don't have to write the rule 4 times.
\end{definition}

\begin{remark}{{(Type Theoretic Substitution)}}
    Suppose we have $\Gamma,x:A,y_1:B_1,\dots,y_n:B_n \vdash C\type $ and an element $a:A$ in context $\Gamma$, we can substitute $a$ for all occurrences of $x$ in types $B_1,\dots,B_n$ and $C$, to get:
    \begin{equation*}
        \Gamma,y_1:B_1[a/x],\dots,y_n:B_n[a/x]\vdash C[a/x]\type 
    \end{equation*}
    The variable $y_1,\dots,y_n$ are assigned new types. Also, we can substitute $a$ for $x$ in a element $c:C$ to obtain $c[a/x]:C[a/x]$. As this can be done on both sides together, even in the case with judgmental equality, we have the substitution rules.
\end{remark}

\begin{definition}{\textbf{(Substitution Rules)}}
    These rules assert that we can do the substitution on all kinds of judgement and on the whole context:
    \begin{equation*}
        \frac{\Gamma\vdash a:A \qquad \Gamma, x:A,\Delta\vdash\mathcal{J}}{\Gamma,\Delta[a/x]\vdash\mathcal{J}[a/x]}S
    \end{equation*}
    Furthermore, we have to make sure that substitution of equal terms lead to equal substituted terms i.e congruent under both the judgemental equal kinds:
    \begin{equation*}
    \begin{aligned}
        \frac{\Gamma\vdash a\equiv a':A\qquad \Gamma,x:A,\Delta\vdash B\type }{\Gamma,\Delta[a/x]\vdash B[a/x]\equiv B[a'/x]\type } \qquad \qquad \frac{\Gamma\vdash a\equiv a':A \qquad \Gamma,x:A,\Delta\vdash b:B}{\Gamma,\Delta[a/x]\vdash b[a/x]\equiv b[a'/x]:B[a'/x]}
    \end{aligned}
    \end{equation*}
    Observe that both $B[a/x]$ and $B[a'/x]$ are types in context $\Delta[a/x]$ provided that $a\equiv a'$.
\end{definition}

\begin{definition}{\textbf{(Fiber)}}
    There are 2 cases, when consider the substitution on type family and its element. When $B$ is a family of types over $A$ in context $\Gamma$ and if $a:A$, then we say that $B[a/x]$ is a fiber of $B$ at $a$, denoted as $B(a)$. On the other hand, When $b$ is a section of the family $B$ over $A$ in context $\Gamma$, we call the element $b[a/x]$ the value of $b$ at $a$, denoted as $b(a)$
\end{definition}

\begin{remark}{(Relationship between Section and Fiber)}
    \textbf{(NOTE: Not sure)} Fiber is more of renaming, while section is more of like evaluation.
    
    For example (not accurate but illustrate the points), the type family is $f(x, y)= x^2+y^2$. Then the fiber of $f$ at $a$ is $f(x, y)[a/y]=f(x, a)=x^2+a^2$. Its section (an element of type families) is $f(10, y)=10^2+y^2$, while fiber of section is $f[a/y](10, y)$. The indexed section can be $f(10, 3)=10^2+3^2$.
\end{remark}

\begin{definition}{\textbf{(Weaken Rule)}}
    Weakening by a type $A$ in context preserves well-formedness and judgmental equality, where we have:
    \begin{equation*}
        \frac{\Gamma\vdash A\type \qquad \Gamma,\Delta\vdash\mathcal{J}}{\Gamma,x:A,\Delta\vdash\mathcal{J}}W
    \end{equation*}
    The process is expanding the context by a variable of type $A$ is called weakening (by $A$). In the simplest situation when weakening applies, we have $2$ types of $A$ and $B$ in context $\Gamma$.   
\end{definition}

\begin{definition}{\textbf{(Constant/Trivial Family)}}
    The simplest example of weakening is to weaken $B$ by $A$ as:
    \begin{equation*}
        \frac{\Gamma\vdash A\type \qquad \Gamma\vdash B\type }{\Gamma,x:A\vdash B\type }
    \end{equation*}
    The type $B$ in context $\Gamma,x:A$ is called constant family $B$ or the trivial family $B$.
\end{definition}

Given a type $A$ in context $\Gamma$, then we can weaken $A$ itself to obtain $A$ is a type in context $\Gamma,x:A$. 

\begin{definition}{\textbf{(Variable Rule/Generic Element)}}
    Asserts that any element $x:A$ in the context $\Gamma,x:A$ is also an element of type $A$ in context $\Gamma,x:A$:
    \begin{equation*}
        \frac{\Gamma\vdash A\type }{\Gamma,x:A\vdash x:A}\delta
    \end{equation*}
    It is to make sure that the variable declared in a context are indeed elements and provides identifying function on the type $A$ in context $\Gamma$.
\end{definition}

\subsection{Derivation}

\begin{definition}{\textbf{(Derivation)}}
    Aa finite tree in which each node is a valid rule of inference. The root is the conclusion and the leaves of the tree we find the hypothesis. Given a derivation with hypothesis $\mathcal{H}_1,\dots,\mathcal{H}_n$ and conclusion $\mathcal{C}$, we have a new inference rule as:
    \begin{equation*}
        \frac{\mathcal{H}_1,\dots,\mathcal{H}_n}{\mathcal{C}}
    \end{equation*}
    such a rule is called derivative. To make things short, we can use any derived rules in any future derivation.
\end{definition}

\begin{lemma}{\textbf{(Changing Variable)}}
    Variable can always be change to fresh variable, given as:
    \begin{equation*}
        \frac{\Gamma,x:A,\Delta\vdash\mathcal{J}}{\Gamma,x':A,\Delta[x'/x]\vdash\mathcal{J}[x'/x]}x'/x
    \end{equation*}
    where $x'$ doesn't occurs in context $\Gamma,x:A,\Delta$
\end{lemma}
\begin{dem}
\begin{proof}
    Note that we have to do weakening rule is because we have to make sure that the context matches before performing the substitution
    \begin{equation*}
        \cfrac{\cfrac{\Gamma\vdash A\type }{\Gamma, x':A\vdash x':A}\ \delta \qquad \cfrac{\Gamma\vdash A\type \qquad \Gamma,x:A,\Delta\vdash\mathcal{J}}{\Gamma,x':A,x:A,\Delta\vdash\mathcal{J}}\ W}{\Gamma,x':A,\Delta[x'/x]\vdash\mathcal{J}[x'/x]}\ S
    \end{equation*}
\end{proof}
\end{dem}

\begin{lemma}{\textbf{(Interchanging Variables)}}
    If we have 2 types $A$ and $B$ in context $\Gamma$ and make a judgement in context $\Gamma,x:A,y:B,\Delta$ then we make that same judgement in context $\Gamma,y:B,x:A,\Delta$
    \begin{equation*}
        \cfrac{\Gamma\vdash B\type  \qquad \Gamma,x:A,y:B,\Delta\vdash\mathcal{J}}{\Gamma,y:B,x:A,\Delta\vdash\mathcal{J}}
    \end{equation*}
\end{lemma}
\begin{dem}
\begin{proof}
    The trick is that the substitution two times can leads to the ``deletion'' of the elements (this is how we delete $y':B$ at the end, by replacing it again with $y$):
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\Gamma\vdash B\type }{\Gamma,y:B\vdash y:B}\ \delta}{\Gamma,y:B,x:A\vdash y:B} \ W \qquad\cfrac{\cfrac{\Gamma,x:A,y:B,\Delta\vdash\mathcal{J}}{\Gamma\vdash B\type \qquad\Gamma,x:A,y':B,\Delta[y'/y]\vdash\mathcal{J}[y'/y]}\ y'/y}{\Gamma,y:B,x:A,y':B,\Delta[y'/y]\vdash\mathcal{J}[y'/y]} \ W}{\Gamma,y:B,x:A,\Delta\vdash\mathcal{J}} \ S
    \end{equation*}
\end{proof}
\end{dem}

\begin{lemma}{\textbf{(Element Conversion Rule)}}
    \label{lemma:element-conv-rule}
    If the types are judgmentally equals to each other then we can interchange the type of its element:
    \begin{equation*}
        \cfrac{\Gamma\vdash A\equiv A'\type \qquad\Gamma\vdash a:A}{\Gamma\vdash a:A'} \qquad \quad \cfrac{\Gamma\vdash A\equiv A'\type\qquad \Gamma\vdash a\equiv b:A}{\Gamma\vdash a\equiv b:A'}
    \end{equation*}
    On the RHS, we also have the congruence rule for element conversion.
\end{lemma}
\begin{dem}
\begin{proof}
    For the first part:
    \begin{equation*}
        \cfrac{\cfrac{}{\Gamma\vdash a:A}  \qquad \cfrac{\cfrac{\Gamma\vdash A\equiv A'\type }{\Gamma\vdash A'\equiv A\type }\qquad \cfrac{\Gamma\vdash A'\type }{\Gamma, x:A'\vdash x:A'}\ \delta}{\Gamma, x:A\vdash x:A'}}{\Gamma\vdash a:A'} \ S
    \end{equation*}
    For the second part, we have used the congruence under substitution, at the last step.
    \begin{equation*}
        \cfrac{\cfrac{}{\Gamma\vdash a\equiv b:A}\qquad \cfrac{\cfrac{\Gamma\vdash A\equiv A'\type}{\Gamma\vdash A'\equiv A\type}\qquad\cfrac{\Gamma\vdash A'\type}{\Gamma, x:A'\vdash x:A'}\ \delta}{\Gamma,x:A\vdash x:A'}}{\Gamma\vdash a\equiv b:A'}
    \end{equation*}
\end{proof}
\end{dem}


\subsection{Dependent Function Types}

\begin{definition}{\textbf{(Dependent Function Types)}}
    Consider section $b$ of family $B$ over $A$ in context $\Gamma$ i.e $\Gamma,x:A\vdash b(x):B(x)$ such section $b$ is an operator that takes as input $x:A$ and produces a term $b(x):B(x)$, which may depend on $x:A$. We will denote type of dependent functions as $\Pi_{(x:A)}B(x)$.
\end{definition}

\begin{remark}{\textbf{(Principle Rules for $\Pi$-Types)}}
    There are 4 principal rules for this: (1) The formulation rule (2) The introduction rule (3) The elimination rule (4) The computation rule. 
    
    Apart from that we also need rules, as part of the specification, that assert that all the constructors respect judgemental equality, called congruence rules
\end{remark}

\begin{definition}{\textbf{($\Pi$-Formation Rule)}}
    Tells us how we may form dependent function type. For any type family $B$ of types over $A$, $\Pi$-formation says (LHS), and it should respect judgmental equality (RHS):
    \begin{equation*}
        \frac{\Gamma,x:A\vdash B(x)\type }{\Gamma\vdash\Pi_{(x:A)}B(x)\type }\ \Pi
        \qquad\qquad
        \cfrac{\Gamma\vdash A\equiv A'\type  \qquad \Gamma, x:A\vdash B(x)\equiv B'(x)\type }{\Gamma\vdash\Pi_{(x:A)}B(x)\equiv\Pi_{(x:A')}B'(x)\type }\ \Pi\text{-eq}
    \end{equation*}
    In order to form the type $\Pi_{(x:A)}B(x)$ in context $\Gamma$, we must have a type family $B$ over $A$ in context $\Gamma$.
\end{definition}

\begin{definition}{\textbf{($\Pi$-Introduction Rule)}}
    Tells us how to introduce new term of dependent function types. A dependent function $f:\Pi_{(x:A)}B(x)$ is an operation that takes an $x:A$ to $f(x):B(x)$. Or, in order to construct a dependent function, one has to construct a term $b(x):B(x)$ indexed by $x:A$ in context $\Gamma$ (LHS), and it should respect judgmental equality (RHS):
    \begin{equation*}
        \cfrac{\Gamma,x:A\vdash b(x):B(x)}{\Gamma\vdash\lambda x.b(x):\Pi_{(x:A)}B(x)}\ \lambda
        \qquad\qquad 
        \cfrac{\Gamma,x:A\vdash b(x)\equiv b'(x):B(x)}{\Gamma\vdash\lambda x.b(x)\equiv \lambda x.b'(x):\Pi_{(x:A)}B(x)}\ \lambda\text{-eq}
    \end{equation*}
    This is also called $\lambda$-abstraction rule, and say that the $\lambda x.b(x)$ binds the variable $x$ in $b$. 
\end{definition}

\begin{definition}{\textbf{($\Pi$-Elimination Rule)}}
    Tells us how to use arbitrary terms of dependent function types. In other words, it is an evaluate rule (LHS) and it should respect judgmental equality (RHS):
    \begin{equation*}
        \cfrac{\Gamma\vdash f: \Pi_{(x:A)}B(x)}{\Gamma, x:A\vdash f(x):B(x)} \ \text{ev}
        \qquad \qquad
        \cfrac{\Gamma\vdash f\equiv f':\Pi_{(x:A)}B(x)}{\Gamma, x:A\vdash f(x)\equiv f'(x):B(x)} \ \text{ev-eq}
    \end{equation*}
\end{definition}

\begin{definition}{\textbf{($\Pi$-Computation Rule)}}
    Tells us how the introduction and elimination rule interact i.e guarantee that every term of a dependent function type is indeed a dependent function taking the values by which it is defined. On LHS, When we evaluate it at $x:A$, we obtain the value $b(x):B(x)$ (called $\beta$-rule):
    \begin{equation*}
         \cfrac{\Gamma, x:A\vdash b(x):B(x)}{\Gamma,x:A\vdash\big(\lambda y.b(y)\big)(x)\equiv b(x):B(x)}\ \beta \qquad \qquad \cfrac{\Gamma\vdash f:\Pi_{(x:A)}B(x)}{\Gamma\vdash \lambda x.f(x)\equiv f : \Pi_{(x:A)}B(x)}\ \eta
    \end{equation*}
    On RHS, we asserts that all elements of a $\Pi$-Type are dependent function (called $\eta$-rule). In other words, $\lambda$-abstraction rules and evaluate rules are mutual inverse
\end{definition}

\begin{definition}{\textbf{(Ordinary Function Type)}}
    Consider the case where both $A$ and $B$ are types in context $\Gamma$, and construct the ordinary function from $A$ to $B$:
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\Gamma\vdash A\type \qquad\Gamma\vdash B\type }{\Gamma, x:A\vdash B\type }\ W}{\Gamma\vdash\Pi_{(x:A)}B\type }\ \Pi}{\Gamma\vdash A\rightarrow B:=\Pi_{(x:A)}B\type }
    \end{equation*}
\end{definition}

\begin{remark}
    We can make definition at the end of a derivation if the conclusion is a certain type/term in context. Suppose we have derivation of (LHS)
    \begin{equation*}
        \cfrac{\mathcal{D}}{\Gamma\vdash a:A} \qquad \qquad \cfrac{\cfrac{\mathcal{D}}{\Gamma\vdash a:A}}{\Gamma\vdash c:= a:A}
    \end{equation*}
    in which the derivation make use of the premises $\mathcal{H}_1,\dots,\mathcal{H}_n$. If we wish to make a definition $c:=a$, then we can extend the derivation tree (RHS). The effect is that we have extend our type theory with a new constant $c$ for which the inference rules are valid:
    \begin{equation*}
        \cfrac{\mathcal{H}_1\quad\mathcal{H}_2\quad\cdots\quad\mathcal{H}_n}{\Gamma\vdash c:A} \qquad \qquad
        \cfrac{\mathcal{H}_1\quad\mathcal{H}_2\quad\cdots\quad\mathcal{H}_n}{\Gamma\vdash c\equiv a:A} 
    \end{equation*}
    This would works with the definition of the ordinary function type that is we have $\Gamma\vdash A\rightarrow B\equiv\Pi_{(x:A)}B\type $, at the end of the derivation.
\end{remark}

We can now use the  term conversion rules, we have the corresponding formation, introduction, elimination and computation rules for the ordinary function type.

\begin{equation*}
    \cfrac{\Gamma\vdash A\type\qquad \Gamma\vdash B\type}{\Gamma\vdash A\to B\type} \ \to \qquad\qquad \cfrac{\Gamma\vdash B\type\qquad \Gamma, x:A\vdash b(x):B\type}{\Gamma\vdash\lambda x.b(x) :A\to B}\ \lambda
\end{equation*}

and so on. Now, we will consider identity function and the composition between functions. 

\begin{definition}{\textbf{(Identity Function)}}
    For any type $A$ in context $\Gamma$, we define the identity function $\operatorname{id}_A:A\rightarrow A$ using the generic terms as: 
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\Gamma\vdash A\type}{\Gamma, x:A\vdash x:A}}{\Gamma\vdash \lambda x.x:A\rightarrow A}\ \lambda}{\Gamma\vdash\operatorname{id}_A:= \lambda x.x:A\rightarrow A}
    \end{equation*}
    And, satisfies the following inference rules as:
    \begin{equation*}
        \cfrac{\Gamma\vdash A \type }{\Gamma\vdash\operatorname{id}_A:A\rightarrow A} \qquad \qquad \cfrac{\Gamma\vdash A\type }{\Gamma\vdash\operatorname{id}_A\equiv\lambda x.x:A\rightarrow A}
    \end{equation*}
\end{definition}

For composition of function, we will introduce composition itself as a function $\operatorname{comp}$ that takes $2$ arguments: $g:B\rightarrow C$ and $f:A\rightarrow B$. The output is  $\operatorname{comp}(g,f):A\rightarrow C$ for which we often write $g\circ f$. 

\begin{remark}{(Observation on Multiple-Valued Function)}
    For multiple argument function, we can form it by  iterating the $\Pi$-formation rule or the $\rightarrow$-formation rule. For example, with function $f:A\rightarrow(B\rightarrow C)$. 
    
    On the other hand, When $C(x,y)$ being a family of types indexed by $x:A$ and $y:B(x)$, then we can form the dependent type as: $\Pi_{(x:A)}\Pi_{(y:B(x))}C(x,y)$. In the special case where $C(x,y)$ is a family of types indexed by 2 elements $x,y:A$ of the same type, then we often write: $\Pi_{(x,y):A}C(x,y)$ for the type $\Pi_{(x:A)}\Pi_{(y:A)}C(x,y)$.
\end{remark}

\begin{definition}
    Given types $A,B$ and $C$ in context $\Gamma$, there is a composition operation:
    \begin{equation*}
        \operatorname{comp}:(B\rightarrow C)\rightarrow\big( (A\rightarrow B)\rightarrow (A\rightarrow C) \big)
    \end{equation*}
    It can also be defined as: $\operatorname{comp} := \lambda g.\lambda f.\lambda x.g(f(x))$, in which the construction is given by:
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\cfrac{\cfrac{\cfrac{\Gamma\vdash A\type\qquad\Gamma\vdash B\type}{\Gamma,f:B^A,x:A\vdash f(x):B} \ (1)}{\Gamma, g:C^B,f:B^A,x:A\vdash f(x):B} \ (W) \qquad \cfrac{\cfrac{\cfrac{\Gamma\vdash B\type\qquad\Gamma\vdash C\type}{\Gamma,g:C^B, y:B\vdash g(y):C} \ (2)}{\Gamma,g:C^B,x:A, f:B^A, y:B\vdash g(y):C}\ (W)}{\Gamma,g:C^B,f:B^A, x:A, y:B\vdash g(y):C}\ (W)}{\Gamma,g:C^B,f:B^A, x:A\vdash g(f(x)):C} \ (E)}{\Gamma, g:C^B, f:B^A\vdash\lambda x.g\big(f(x)\big):C^A}}{\Gamma, g:B\to C\vdash\lambda f.\lambda x.g\big(f(x)\big):B^A\to C^A}}{\Gamma\vdash\operatorname{comp}:=\lambda g.\lambda f.\lambda x.g\big(f(x)\big):C^B\rightarrow\big(B^A\rightarrow C^A\big)}
    \end{equation*}
    where the last 3 step we just used the lambda rule $(\lambda)$. For the derivation of $(1)$ and $(2)$, we have:
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\Gamma\vdash A\type\qquad\Gamma\vdash B\type}{\Gamma\vdash A\to B\type}}{\Gamma, f:B^A\vdash f:B^A}\ \delta}{\Gamma, f:B^A, x:A \vdash f(x):B} \ \text{ev}
    \end{equation*}
\end{definition}

And so we have that function composition is associative and that the identity function satisfies the unit laws.

\begin{lemma}
    The composition of function is associative:
    \begin{equation*}
        \cfrac{\Gamma\vdash f:A\rightarrow B \qquad \Gamma\vdash g:B\rightarrow C\qquad \Gamma\vdash h:C\rightarrow D}{\Gamma\vdash \big(h\circ g\big)\circ f\equiv h\circ\big(g\circ f\big):A\rightarrow B}
    \end{equation*}
\end{lemma}

\begin{dem}
\begin{proof}
    The idea: both $\big((h\circ g)\circ f\big)(x)$ and $\big( h\circ(g\circ f) \big)(x)$ evaluate to be $h(g(f(x)))$ and therefore $(h\circ g)\circ f$ and $h\circ(g\circ f)$ must be judgmentally equal.
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\cfrac{\cfrac{\cfrac{\cfrac{\Gamma\vdash f:A\to B}{\Gamma, x:A\vdash f(x):B}\text{ ev}\qquad\cfrac{\cfrac{\Gamma\vdash g:B\to C}{\Gamma, y:B\vdash g(y):C}\text{ ev}}{\Gamma, x:A, y:B\vdash g(y):C}\ W}{\Gamma, x:A\vdash g(f(x)):C}\ E\qquad \cfrac{\cfrac{\Gamma\vdash h:C\to D}{\Gamma, z:C\vdash h(z):D}\text{ ev}}{\Gamma, x:A, z:C\vdash b(z):D}\ W}{\Gamma,x:A\vdash h(g(f(x))):D} \ E}{\Gamma,x:A\vdash h(g(f(x)))\equiv h(g(f(x))):D}}{\Gamma,x:A\vdash (h\circ g)(f(x))\equiv h((g\circ f)(x)):D}}{\Gamma,x:A\vdash ((h\circ g)\circ f)(x)\equiv (h\circ(g\circ f))(x):D}\ \Pi}{\Gamma\vdash ((h\circ g)\circ f)(x)\equiv (h\circ(g\circ f))(x):A\to D}
    \end{equation*}
    Note that the last two steps were just the untangling of the definition of composition between functions.
\end{proof}
\end{dem}

\begin{lemma}
    Composition of functions satisfy the left and right unit law i.e we can derive as:
    \begin{equation*}
        \cfrac{\Gamma\vdash f:A\rightarrow B}{\Gamma\vdash\operatorname{id}_B\circ f\equiv f:A\rightarrow B} \qquad\qquad \cfrac{\Gamma\vdash f:A\rightarrow B}{\Gamma\vdash f\circ \operatorname{id}_A\equiv f:A\rightarrow B}
    \end{equation*}
\end{lemma}
\begin{dem}
\begin{proof}
    It suffices to derive that $\operatorname{id}_B(f(x))\equiv f(x)$ in context $\Gamma,x:A$ 
    \begin{equation*}
        \cfrac{\cfrac{\Gamma\vdash f:A\rightarrow B}{\Gamma, x:A\vdash f(x):B}\text{ ev}\qquad \cfrac{\cfrac{}{\Gamma\vdash A\type}\qquad \cfrac{\cfrac{\cfrac{\Gamma\vdash B\type}{\Gamma, y:B\vdash y:B}\ \delta}{\Gamma, y:B\vdash(\lambda x.x)(y)\equiv y}\ \beta\qquad\cfrac{\cfrac{\Gamma\vdash B\type }{\Gamma\vdash\operatorname{id}_B\equiv\lambda x.x:B\rightarrow B}}{\Gamma, y:B\vdash \operatorname{id}_B(y)\equiv \lambda x.x(y):B}\text{ ev-eq}}{\Gamma, y:B\vdash \operatorname{id}_B(y)\equiv y:B}}{\Gamma, x:A, y:B\vdash \operatorname{id}_B(y)\equiv y:B}\ W}{\Gamma,x:A\vdash\operatorname{id}_B(f(x))\equiv f(x):B} \ E
    \end{equation*}
    Then we have that:
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\vdots}{\Gamma,x:A\vdash\operatorname{id}_B(f(x))\equiv f(x):B}}{\Gamma\vdash\lambda x.\operatorname{id}_B(f(x))\equiv\lambda x.f(x):A\to B}\ \lambda\text{-eq}\qquad\cfrac{\Gamma\vdash f:A\to B}{\Gamma\vdash\lambda x.f(x)\equiv f:A\to B}\ \eta}{\Gamma:\operatorname{id}_B\circ f\equiv f:A\rightarrow B}
    \end{equation*}
\end{proof}
\end{dem}

\begin{lemma}
    \label{lemma:equal-function-eval-equal}
    If $f$ and $g$ are equal values, then they must be equal that is:
    \begin{equation*}
        \cfrac{\Gamma\vdash f:\Pi_{(x:A)}B(x)\qquad \Gamma\vdash g:\Pi_{(x:A)}B(x)\qquad \Gamma, x:A\vdash f(x)\equiv g(x):B(x)}{\Gamma\vdash f\equiv g:\Pi_{(x:A)}B(x)}
    \end{equation*}
\end{lemma}
\begin{dem}
\begin{proof}
    We have that:
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\Gamma\vdash f:\Pi_{(x:A)}B(x)}{\Gamma\vdash \lambda x.f(x)\equiv f:\Pi_{(x:A)}B(x)}\ \eta\qquad\cfrac{\Gamma, x:A\vdash f(x)\equiv g(x):B(x)}{\Gamma\vdash \lambda x.f(x)\equiv \lambda x.g(x):\Pi_{(x:A)}B(x)}\ \lambda\text{-eq}}{\Gamma\vdash f\equiv\lambda x.g(x):\Pi_{(x:A)}B(x)}\qquad\cfrac{\Gamma\vdash g:\Pi_{(x:A)}B(x)}{\Gamma\vdash \lambda x.g(x)\equiv g:\Pi_{(x:A)}B(x)}\ \eta}{\Gamma\vdash f\equiv g:\Pi_{(x:A)}B(x)}
    \end{equation*}
\end{proof}
\end{dem}

\begin{definition}{\textbf{(Constant Function)}}
    \label{def:const-funct}
    We can derive the constant function as follows:
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\cfrac{}{\Gamma\vdash A\type}\qquad\cfrac{\Gamma\vdash B\type}{\Gamma, y:B\vdash y:B}\ \delta}{\Gamma, y:B, x:A\vdash y:B}\ W}{\Gamma, y:B\vdash \lambda x.y:A\to B}\ \lambda}{\Gamma, y:B\vdash\operatorname{const}_y:=\lambda x.y:A\to B}\quad \rightsquigarrow\quad \cfrac{\Gamma\vdash A\type}{\Gamma, y:B\vdash\operatorname{const}_y:A\to B}
    \end{equation*}
\end{definition}

\begin{lemma}
    \label{lemma:const-funct-compose}
    The constant function behaves as expected that is:
    \begin{equation*}
        \cfrac{\Gamma\vdash f:A\to B}{\Gamma, z:C\vdash \operatorname{const}_z\circ f\equiv\operatorname{const}_z:A\to C} \qquad \cfrac{\Gamma\vdash A\type \qquad \Gamma\vdash g:B\to C}{\Gamma, y:B\vdash g\circ \operatorname{const}_y\equiv\operatorname{const}_{g(y)}:A\to C}
    \end{equation*}
\end{lemma}
\begin{dem}
\begin{proof}{\textbf{(First Part):}}
    First, we are trying to perform the evaluation of the lambda function:
    \begin{equation*}
        \cfrac{
            \cfrac{\cfrac{\Gamma\vdash B\type \qquad \Gamma\vdash C\type}{\Gamma, z:C\vdash\operatorname{const}_z\equiv\lambda y.z:B\to C}}{\Gamma, y:B, z:C\vdash\operatorname{const}_z(y)\equiv\lambda y.z(y):C}\text{ ev-eq}
            \qquad \cfrac{\cfrac{\cfrac{}{\Gamma\vdash B\type}\qquad \cfrac{\Gamma\vdash C\type}{\Gamma,z:C\vdash z:C} \ \delta}{\Gamma, z:C, y:B\vdash z:C} \ W}{\Gamma, z:C, y:B\vdash(\lambda y.z)(y)\equiv z:C}\ \beta
        }{\Gamma, y:B, z:C\vdash\operatorname{const}_z(y)\equiv z:C} \ (\text{swap}+E)
    \end{equation*}
    Then from this, we have that:
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\Gamma\vdash f:A\to B}{\Gamma, x:A\vdash f(x):B}\ \text{ev}\qquad \cfrac{\cfrac{}{\Gamma\vdash A\type}\qquad \cfrac{\vdots}{\Gamma, y:B, z:C\vdash\operatorname{const}_z(y)\equiv z:C}}{\Gamma, x:A, y:B, z:C\vdash\operatorname{const}_z(y)\equiv z:C} \ W}{\Gamma, x:A,z:C\vdash\operatorname{const}_z(f(x))\equiv z:C} \ E}{\Gamma,z:C\vdash\lambda x.\operatorname{const}_z(f(x))\equiv\lambda x.z:A\to C}\ (\text{swap}+\lambda\text{-eq})
    \end{equation*}
    Finally, we have that:
    \begin{equation*}
        \cfrac{\cfrac{\vdots}{\Gamma,z:C\vdash\lambda x.\operatorname{const}_z(f(x))\equiv\lambda x.z:A\to C}\qquad\cfrac{\Gamma\vdash A\type}{\Gamma, z:C\vdash\operatorname{const}_z\equiv\lambda x.z:A\to C}}{\Gamma, z:C\vdash\operatorname{const}_z\circ f\equiv\operatorname{const}_z:A\to C}
    \end{equation*}
\end{proof}
\end{dem}
\begin{dem}
\begin{proof}{\textbf{(Second Part):}}
    We have that:
    \begin{equation*}
        \cfrac{\cfrac{\Gamma\vdash g:B\to C}{\Gamma, y:B\vdash g(y):C}\text{ ev}\qquad \cfrac{\cfrac{}{\Gamma\vdash B\type}\qquad\cfrac{\Gamma\vdash A\type}{\Gamma, z:C\vdash\operatorname{const}_z\equiv\lambda x.z:A\to C}}{\Gamma, y:B, z:C\vdash\operatorname{const}_z\equiv\lambda x.z:A\to C}}{\Gamma, y:B\vdash\operatorname{const}_{g(y)}\equiv\lambda x.g(y):A\to C} \ E
    \end{equation*}
    We aim to show that $\Gamma, y:B \vdash g\circ\operatorname{const}_y\equiv\lambda x.g(y)$. Thus, we have that (from the first part, together with the congruence of substitution rule, for the last step, assuming we can duplicate the context $\dots,y:B,y:B\vdash\dots$):
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\vdots}{\Gamma, x:A,y:B\vdash\operatorname{const}_y(x)\equiv y:B}\qquad\cfrac{\cfrac{}{\Gamma\vdash A\type}\qquad \cfrac{\Gamma\vdash g:B\to C}{\Gamma, y:B\vdash g(y):C}\text{ ev}}{\Gamma, x:A, y:B\vdash g(y):C}\ W}{\Gamma, x:A, y:B\vdash g(\operatorname{const}_y(x))\equiv g(y):C}}{\Gamma, y:B\vdash\lambda x.g(\operatorname{const}_y(x))\equiv\lambda x.g(y):A\to C}\ \lambda\text{-eq}
    \end{equation*}
    So we have that:
    \begin{equation*}
        \cfrac{\cfrac{\vdots}{\Gamma, y:B\vdash\lambda x.g(\operatorname{const}_y(x))\equiv\lambda x.g(y):A\to C}\qquad\cfrac{\vdots}{\Gamma, y:B\vdash\operatorname{const}_{g(y)}\equiv\lambda x.g(y):A\to C}}{\Gamma, y:B\vdash (g\circ\operatorname{const}_y)\equiv \operatorname{const}_{g(y)}:C}
    \end{equation*}
\end{proof}
\end{dem}

\begin{definition}{\textbf{(Swap Function)}}
    \label{def:swap-funct}
    We can derive the swap function as follows:
    \begin{equation*}
        \cfrac{\Gamma\vdash A\type \qquad \Gamma\vdash B\type\qquad \Gamma, x:A, y:B\vdash C(x, y)\type}{\Gamma\vdash\sigma:\big( \Pi_{(x:A)}\Pi_{(y:B)}C(x, y) \big)\to\big( \Pi_{(y:B)}\Pi_{(x:A)}C(x, y) \big)}
    \end{equation*}
    We start by a creation of the evaluated elements (in order for us to prepare for the creation of lambda element):
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\cfrac{\cfrac{\cfrac{\Gamma, x:A, y:B\vdash C(x, y)\type}{\Gamma, x:A\vdash \Pi_{(y:B)}C(x, y) \type}\ \Pi}{\Gamma\vdash \Pi_{(x:A)}\Pi_{(y:B)}C(x, y)\type}\ \Pi}{\Gamma, f:\Pi_{(x:A)}\Pi_{(y:B)}C(x, y)\vdash f:\Pi_{(x:A)}\Pi_{(y:B)}C(x, y)}\ \delta}{\Gamma, f:\Pi_{(x:A)}\Pi_{(y:B)}C(x, y), x:A\vdash f(x):\Pi_{(y:B)}C(x, y)}\text{ ev}}{\Gamma, f:\Pi_{(x:A)}\Pi_{(y:B)}C(x, y), x:A, y:B\vdash f(x, y):C(x, y)}\text{ ev}}{\Gamma, f:\Pi_{(x:A)}\Pi_{(y:B)}C(x, y), y:B, x:A\vdash f(x, y):C(x, y)}\text{ swap}
    \end{equation*}
    Then we have to repeatedly used the lambda rule:
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\cfrac{\Gamma, f:\Pi_{(x:A)}\Pi_{(y:B)}C(x, y), y:B, x:A\vdash f(x, y):C(x, y)}{\Gamma, f:\Pi_{(x:A)}\Pi_{(y:B)}C(x, y), y:B\vdash \lambda x.f(x, y):\Pi_{(x:A)}C(x, y)}}{\Gamma, f:\Pi_{(x:A)}\Pi_{(y:B)}C(x, y)\vdash \lambda y.\lambda x.f(x, y):\Pi_{(y:A)}\Pi_{(x:A)}C(x, y)}}{\Gamma\vdash \lambda f.\lambda y.\lambda x.f(x, y):\big(\Pi_{(x:A)}\Pi_{(y:B)}C(x, y)\big)\to\big(\Pi_{(y:A)}\Pi_{(x:A)}C(x, y)\big)}}{\Gamma\vdash\sigma:=\lambda f.\lambda y.\lambda x.f(x, y):\big(\Pi_{(x:A)}\Pi_{(y:B)}C(x, y)\big)\to\big(\Pi_{(y:A)}\Pi_{(x:A)}C(x, y)\big)}
    \end{equation*}
\end{definition}

\begin{lemma}
    \label{lemma:swap-two-time}
    When we use the swap function 2 times, we ended up getting the same function:
    \begin{equation*}
        \cfrac{\Gamma\vdash A\type \qquad \Gamma\vdash B\type\qquad \Gamma, x:A, y:B\vdash C(x, y)\type
        }{\Gamma\vdash\sigma\circ\sigma\equiv\operatorname{id}:\big( \Pi_{(x:A)}\Pi_{(y:B)}C(x, y) \big)\to\big( \Pi_{(x:A)}\Pi_{(y:B)}C(x, y) \big)}
    \end{equation*}
\end{lemma}
\begin{dem}
\begin{proof}
    Let's start with getting the lambda formulation of $\operatorname{id}$ as we repeated apply the $\eta$ rule (we will also stop using the type annotation because it is too long):
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\cfrac{\vdots}{\Gamma\vdash \Pi_{(x:A)}\Pi_{(y:B)}C(x, y)\type}}{\Gamma\vdash\operatorname{id}\equiv \lambda g.g:\big( \Pi_{(x:A)}\Pi_{(y:B)}C(x, y)\big)\to\big( \Pi_{(x:A)}\Pi_{(y:B)}C(x, y)\big)}}{\Gamma, f:\Pi_{(x:A)}\Pi_{(y:B)}C(x, y)\vdash\operatorname{id}(f)\equiv \lambda g.g(f)}\text{ ev-eq}\qquad\cfrac{\cfrac{\cfrac{\vdots}{\Gamma\vdash\Pi_{(x:A)}\Pi_{(y:B)}C(x, y)\type}}{\Gamma, f\vdash f} \ \delta}{\Gamma, f\vdash (\lambda g.g)(f)\equiv f}\ \beta}{\Gamma ,f\vdash\operatorname{id}(f)\equiv f}
    \end{equation*}
    Note that for the $\beta$ rule on the RHS, we can rename $f$ to $g$ that is because when compare to the rule description $x$ is $f$ and $b(x)$ is $f$ i.e depends fully on the signature of $x$. On the other hand, we have that:

    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\cfrac{\vdots}{\Gamma, f\vdash f\equiv \lambda x.\lambda y.f(x, y)} \qquad \cfrac{\vdots}{\Gamma, f\vdash \operatorname{id}(f)\equiv f}}{\Gamma, f\vdash \operatorname{id}(f)\equiv \lambda x.\lambda y.f(x, y)}}{\Gamma\vdash\lambda f.\operatorname{id}(f)\equiv \lambda f.\lambda x.\lambda y.f(x, y)}\ \lambda\text{-eq}\qquad\cfrac{\Gamma\vdash\operatorname{id}}{\Gamma\vdash\lambda f.\operatorname{id}(f)\equiv f}\ \eta}{\Gamma\vdash\operatorname{id}\equiv \lambda f.\lambda x.\lambda y.f(x, y)}
    \end{equation*}
    For $f\equiv \lambda x.\lambda y.f(x, y)$ we just used repeated application of $\eta$ rule. Now, we have finished the first side, let's move to the swap case, we will not to the proof tree here, but give the following judgemental equalities
    \begin{equation*}
    \begin{aligned}
        &\sigma(\sigma(f))\equiv\sigma(\lambda y'.\lambda x'.f(x', y'))\equiv\lambda x.\lambda y.\big[(\lambda y'.\lambda x'.f(x', y'))(y, x)\big] \\
        &\sigma(\sigma(f))(x'', y'') \equiv \lambda x.\lambda y.\big[(\lambda y'.\lambda x'.f(x', y'))(y, x)\big](x'', y'')\equiv (\lambda y'.\lambda x'.f(x', y'))(y'', x'') \equiv f(x'', y'') \\
    \end{aligned}
    \end{equation*}
    Note that the first line represents the unwrapping of $\lambda$-definition of $\sigma$, while the second line represents the repeated $\beta$ application. Thus we have that (with renaming)
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\cfrac{\cfrac{\vdots}{\Gamma, f, x:A, y:B \vdash\sigma(\sigma(f))(x, y)\equiv f(x, y)}}{\Gamma, f \vdash \lambda x.\lambda y.\sigma(\sigma(f))(x, y)\equiv \lambda x.\lambda y.f(x, y)}\ \lambda\text{-eq}\qquad \cfrac{\cfrac{\cfrac{\cfrac{\vdots}{\Gamma, f\vdash \sigma(\sigma(f))}}{\Gamma, f\vdash \lambda y.\sigma(\sigma(f))(y)\equiv \sigma(\sigma(f))} \ \eta}{\Gamma, f\vdash \lambda x.\lambda y.\sigma(\sigma(f))(y)(x)\equiv \sigma(\sigma(f))}\ \eta}{\Gamma, f\vdash \lambda x.\lambda y.\sigma(\sigma(f))(x, y)\equiv \sigma(\sigma(f))}}{\Gamma, f\vdash \sigma(\sigma(f))\equiv\lambda x.\lambda y.f(x, y)}}{\Gamma\vdash\lambda f.\sigma(\sigma(f))\equiv\lambda f.\lambda x.\lambda y.f(x, y)}\ \lambda\text{-eq}}{\Gamma\vdash \lambda f.(\sigma\circ\sigma)(f)\equiv\lambda f.\lambda x.\lambda y.f(x, y)}
    \end{equation*}
    \todo We have to check the order of the multi-variable function on the RHS. Thus, we have that:
    \begin{equation*}
        \cfrac{\cfrac{\vdots}{\Gamma\vdash\operatorname{id}\equiv\lambda f.\lambda x.\lambda y.f(x, y)}\qquad\cfrac{\vdots}{\Gamma\vdash \lambda f.(\sigma\circ\sigma)(f)\equiv\lambda f.\lambda x.\lambda y.f(x, y)}\qquad\cfrac{\cfrac{\vdots}{\Gamma\vdash \sigma\circ\sigma}}{\Gamma\vdash \lambda f.(\sigma\circ\sigma)(f)\equiv \sigma\circ\sigma}\eta}{\Gamma\vdash\sigma\circ\sigma\equiv\operatorname{id}}
    \end{equation*}
    Note that the RHS we have created the $\sigma\circ\sigma$ from the composition of swap functions. Thus we have the equality as needed.
\end{proof}
\end{dem}

\subsection{Natural Number}

Now, we are ready to consider the formal specification of the type of natural numbers $\mathbb{N}$, which is archetypal example of an inductive type. Similar to dependent function there are 4 rules (Formation, Introduction, Elimination (induction), Computation)

\begin{definition}{\textbf{(Formation Rule of $\mathbb{N}$)}}
    Assert how the type $\mathbb{N}$ can be formed where $\mathbb{N}$ is to be a type in the empty context, as we have:
    \begin{equation*}
        \cfrac{}{\vdash \mathbb{N}\type}\ \mathbb{N}\text{-form}
    \end{equation*}
\end{definition}

\begin{definition}{\textbf{(Introduction Rules of $\mathbb{N}$)}}
    Peano' first axiom postulates that $0$ is a natural number. The introduction rule gives zero-element and successor function:
    \begin{equation*}
        \cfrac{}{\vdash0_\mathbb{N}:\mathbb{N}} \qquad\qquad \cfrac{}{\vdash\succnat:\mathbb{N}\rightarrow\mathbb{N}}
    \end{equation*}
\end{definition}

The classical induction principle of the natural number tells us what we have to do in order to show that $\forall(n\in\mathbb{N})P(n)$ holds for a predicate $P$ over $\mathbb{N}$. 

\begin{definition}\textbf{(Induction Principle of $\mathbb{N}$)}
    We think of type family $P$ over $\mathbb{N}$ as predicate over $\mathbb{N}$ as:
    \begin{equation*}
        \cfrac{\Gamma,n:\mathbb{N}\vdash P(n)\type\qquad\Gamma\vdash p_0:P(0_\mathbb{N})\qquad\Gamma\vdash p_S:\Pi_{(n:\mathbb{N})}P(n)\rightarrow P\big(\succnat(n\big)\big)}{\Gamma\vdash\operatorname{ind}_\mathbb{N}(p_0,p_S):\Pi_{(n:\mathbb{N})}P(n)}
    \end{equation*}
    tells us what is need in order to construct a dependent function $\Pi_{(n:\mathbb{N})}P(n)$, which are base case i.e element $p_0:P(0_\mathbb{N})$ and induction step i.e function of type of $P(n)\rightarrow P(\succnat(n))$ for all $n:\mathbb{N}$.
\end{definition}

\begin{remark}{\textbf{(Alternative Induction Principle)}}
    We just create a functions with both base case and induction step as the input:
    \begin{equation*}
        \cfrac{\Gamma,n:\mathbb{N}\vdash P(n)\type}{\Gamma\vdash\operatorname{ind}_\mathbb{N}:P(0_\mathbb{N})\rightarrow\Big(\big(\Pi_{(n:\mathbb{N})}P(n)\rightarrow P(\succnat(n))\big)\rightarrow\Pi_{(n:\mathbb{N})}P(n)\Big)}
    \end{equation*}
    We can derive this alternative from the original formulation above. Starting with We can construct $\Gamma\vdash P(0_\mathbb{N})\type$ as:
    \begin{equation*}
        \cfrac{\Gamma\vdash 0_\mathbb{N}:\mathbb{N}\qquad\Gamma,n:\mathbb{N}\vdash P(n)\type}{\Gamma\vdash P(0_\mathbb{N})\type}\ S
    \end{equation*}
    Similarly, we can construct $\Pi_{n:\mathbb{N}}\big(P(n)\to P(\succnat(n))\big)\type$ as:
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{}{\Gamma,n:\mathbb{N}\vdash P(n)\type}\qquad \cfrac{\cfrac{\Gamma\vdash\succnat:\mathbb{N}\to \mathbb{N}}{\Gamma, n:\mathbb{N}\vdash\operatorname{succ}(n):\mathbb{N}}\text{ ev}\qquad\cfrac{\Gamma\vdash \mathbb{N}\type\qquad \Gamma, n':\mathbb{N}\vdash P(n')}{\Gamma, n: \mathbb{N}, n':\mathbb{N}\vdash P(n')\type}\ W}{\Gamma, n:\mathbb{N}\vdash P(\succnat(n))\type}\ S}{\Gamma,n:\mathbb{N}\vdash P(n)\to P(\succnat(n))\type}\ \to}{\Gamma\vdash \Pi_{n:\mathbb{N}}\big(P(n)\to P(\succnat(n))\big)\type}\ \Pi
    \end{equation*}
    By the weakening and variable laws, we have the following
    \begin{equation*}
    \begin{aligned}
        &\Gamma,p_0:P(0_\mathbb{N}), p_S:\Pi_{(n:\mathbb{N})}P(n)\rightarrow P\big( \succnat(n) \big)\vdash p_0:P(0_\mathbb{N}) \\
        &\Gamma,p_0:P(0_\mathbb{N}), p_S:\Pi_{(n:\mathbb{N})}P(n)\rightarrow P\big( \succnat(n) \big)\vdash p_S:\Pi_{(n:\mathbb{N})}P(n)\rightarrow P\big( \succnat(n) \big) \\
        &\Gamma,p_0:P(0_\mathbb{N}), p_S:\Pi_{(n:\mathbb{N})}P(n)\rightarrow P\big( \succnat(n) \big), n:\mathbb{N}\vdash P(n)\type \\
    \end{aligned}
    \end{equation*}
    The original induction principle gives us the premise, and we can use the $\lambda$-abstraction twice to obtain a function (as the $\lambda$-abstraction removes the notion of element of the context).
    \begin{equation*}
        \cfrac{\Gamma,p_0:P(0_\mathbb{N}), p_S:\Pi_{(n:\mathbb{N})}P(n)\rightarrow P\big( \succnat(n) \big)\vdash \operatorname{ind}_\mathbb{N}(p_0,p_S):\Pi_{(n:\mathbb{N})}P(n)}{\Gamma\vdash\operatorname{ind}_\mathbb{N}:P(0_\mathbb{N}) \rightarrow \Big( \big( \Pi_{(n:\mathbb{N})}P(n)\rightarrow P(\succnat(n)) \big) \rightarrow \Pi_{(n:\mathbb{N})}P(n) \Big)\ }(\lambda*2)
    \end{equation*}
    On the other hand, the original rule can be derived using the elimination rule and substitution rules (i.e application of the inputs to the given function) as, for example
    \begin{equation*}
        \cfrac{\cfrac{}{\Gamma\vdash p_0:P(0_\mathbb{N})}\qquad\cfrac{\cfrac{\Gamma,n:\mathbb{N}\vdash P(n)\type}{\Gamma\vdash\operatorname{ind}_\mathbb{N}}}{\Gamma, p_0':P(0_\mathbb{N})\vdash\operatorname{ind}_\mathbb{N}(p_0')}\text{ ev}}{\Gamma\vdash\operatorname{ind}_\mathbb{N}(p_0):\big( \Pi_{(n:\mathbb{N})}P(n)\rightarrow P(\succnat(n)) \big) \rightarrow \Pi_{(n:\mathbb{N})}P(n)}\ S
    \end{equation*}
\end{remark}

\begin{definition}{\textbf{(Computation Rule of $\mathbb{N}$)}}
    Asserts that the dependent function: $\operatorname{ind}_\mathbb{N}(p_0,p_S)$ behave as expected when it is applied to a natural number. We start with the base case i.e its behavior at $0_\mathbb{N}$ as:
    \begin{equation*}
        \cfrac{\Gamma,n:\mathbb{N}\vdash P(n)\type\qquad\Gamma\vdash p_0:P(0_\mathbb{N})\qquad\Gamma\vdash p_S:\Pi_{(n:\mathbb{N})}P(n)\rightarrow P\big(\succnat(n\big)\big)}{\Gamma\vdash\operatorname{ind}_\mathbb{N}(p_0,p_S, 0_\mathbb{N})\equiv p_0:P(0_\mathbb{N})}
    \end{equation*}
    On the other hand, the inductive step is given to be:
    \begin{equation*}
        \cfrac{\Gamma,n:\mathbb{N}\vdash P(n)\type\qquad\Gamma\vdash p_0:P(0_\mathbb{N})\qquad\Gamma\vdash p_S:\Pi_{(n:\mathbb{N})}P(n)\rightarrow P\big(\succnat(n\big)\big)}{\Gamma\vdash\operatorname{ind}_\mathbb{N}(p_0,p_S, \succnat(n))\equiv p_S(n, \operatorname{ind}_\mathbb{N}(p_0, p_S, n)):P(\succnat)}
    \end{equation*}
    Note that $\operatorname{ind}_\mathbb{N}(p_0, p_S, n):P(n)$ and $p_S$ can take $2$ arguments.
\end{definition}

\subsection{Addition on the Natural Number}

The induction principle of $\mathbb{N}$ and be used to derive all the familiar properties about natural number. This requires a few more in ingredient of Martin-Lof's dependent type theory. We will need, for example, the identity type to specify all of forming types in Martin-Lof's dependent type.

\begin{definition}{\textbf{(Addition)}}
    We have: $\operatorname{add}_\mathbb{N}:\mathbb{N}\rightarrow\big(\mathbb{N}\rightarrow\mathbb{N}\big)$, satisfying the specification:
    \begin{equation*}
    \begin{aligned} 
        \operatorname{add}_\mathbb{N}(m, 0_\mathbb{N}) &\equiv m \\ \operatorname{add}_\mathbb{N}(m, \succnat(n)) &\equiv \succnat(\operatorname{add}_\mathbb{N}(m,n)) 
    \end{aligned}
    \end{equation*}
    We will write $m+n$ for $\operatorname{add}_\mathbb{N}(m,n)$. This can be constructed by induction on the second variables, follows $m:\mathbb{N}\vdash \operatorname{add}_\mathbb{N}(m):\mathbb{N}\rightarrow\mathbb{N}$. The induction principle of $\mathbb{N}$ is used with the family of type $P(n):=\mathbb{N}$ indexed by $n:\mathbb{N}$ in context $m:\mathbb{N}$:
    \begin{equation*}
    \begin{aligned}
        \cfrac{\cfrac{\cfrac{\vdots}{m:\mathbb{N}\vdash\operatorname{add-zero}_\mathbb{N}(m):=m:\mathbb{N}}\quad\cfrac{\vdots}{m:\mathbb{N}\vdash\operatorname{add-succ}_\mathbb{N}(m):\mathbb{N}\rightarrow(\mathbb{N}\rightarrow\mathbb{N})}}{m:\mathbb{N}\vdash\operatorname{ind}_\mathbb{N}(\operatorname{add-zero}_\mathbb{N}(m), \operatorname{add-succ}_\mathbb{N}(m)):\mathbb{N}\rightarrow\mathbb{N}}}{m:\mathbb{N}\vdash\operatorname{add}_\mathbb{N}(m):=\operatorname{ind}_\mathbb{N}(\operatorname{add-zero}_\mathbb{N}(m),\operatorname{add-succ}_\mathbb{N}(m)):\mathbb{N}\rightarrow\mathbb{N}}
    \end{aligned}
    \end{equation*}
    We can use the specification above to define the functions, in which the element $\operatorname{add-zero}_\mathbb{N}(m):\mathbb{N}$ in context $m:\mathbb{N}$ is $m:\mathbb{N}$. And, $\operatorname{add-succ}_\mathbb{N}(m):\mathbb{N}\rightarrow\big( \mathbb{N}\rightarrow\mathbb{N} \big)$ is $\operatorname{add-succ}_\mathbb{N}(m,n,x)\equiv\succnat(x)$ as we have:
    \begin{equation*}
    \begin{aligned}
        \operatorname{add}_\mathbb{N}(m,\succnat(n)) &\equiv \operatorname{ind}_\mathbb{N}\big( \operatorname{add-zero}_\mathbb{N}(m),\operatorname{add-succ}_\mathbb{N}(m),\succnat(n) \big) \\
        &\equiv \operatorname{add-succ}_\mathbb{N}(m,n,\operatorname{add}_\mathbb{N}(m,n)) \\
        &\equiv \succnat(\operatorname{add}_\mathbb{N}(m,n))
    \end{aligned}
    \end{equation*}
    Note that we have used the computation rule. The formal derivation for the construction of $\operatorname{add-succ}_\mathbb{N}$ is:
    \begin{equation*}
        \cfrac{\cfrac{\cfrac{\cfrac{}{\vdash\mathbb{N}\type}\quad\cfrac{\cfrac{}{\vdash\mathbb{N}\type}\quad\cfrac{}{\vdash\succnat:\mathbb{N}\rightarrow\mathbb{N}}}{n:\mathbb{N}\vdash\succnat:\mathbb{N}\rightarrow\mathbb{N}} \ W}{m:\mathbb{N},n:\mathbb{N}\vdash\succnat:\mathbb{N}\rightarrow\mathbb{N}} \ W}{m:\mathbb{N}\vdash\lambda n.\succnat:\mathbb{N}\rightarrow(\mathbb{N}\rightarrow\mathbb{N})} \ \lambda}{m:\mathbb{N}\vdash\operatorname{add-succ}_\mathbb{N}(m):=\lambda n.\succnat:\mathbb{N}\rightarrow(\mathbb{N}\rightarrow\mathbb{N})}
    \end{equation*}
\end{definition}

\begin{remark}
    We have a recursive definition of the addition. However, we can't show that $0_\mathbb{N}+n\equiv n$ or $\succnat(m)+n=\succnat(n, m)$. Dependent type theory with its inductive types does not provide any means to prove such judgmental equalities, and we will need identity type to do so.
\end{remark}

\begin{definition}{\textbf{(Pattern Matching)}}
    If we want to define a dependent function $f:\Pi_{(n:\mathbb{N})}P(n)$ by induction on $n$ using: $p_0:P(0_\mathbb{N})$ and $p_S:\Pi_{(n:\mathbb{N})}P(n) \rightarrow P(\succnat(n))$, then we can present its definition as:
    \begin{equation*}
    \begin{aligned} 
        &f(0_\mathbb{N}) := p_0 \\ 
        &f(\succnat(n)) := p_S(n,f(n)) 
    \end{aligned}
    \end{equation*}
    To recover the dependent function $p_S$ from the expression $p_S(n,f(n))$, we can replace all occurrence of the term $f(n)$ in the expression with $x:P(n)$. In other words, when a sub-expression of $p_S(n,f(n))$ matches $f(n)$, we replace that sub-expression by $x$.
\end{definition}

\begin{remark}{(Pattern Matchine in $\operatorname{add}(\cdot, \cdot)$)}
    We can observe that we have $p_S:=\operatorname{add-succ}_\mathbb{N}(m, \cdot)$ with $f=\operatorname{add}(m, \cdot)$, then with expression $f(\succnat(n)) := p_S(n,f(n)) $, we have:
    \begin{equation*}
    \begin{aligned}
        \operatorname{add}(m)(\succnat(n)) &:= \operatorname{add-succ}_\mathbb{N}(m)(n)(\operatorname{add}(m)(n)) \\
        &:= \operatorname{succ}(\operatorname{add}(m)(n))
    \end{aligned}
    \end{equation*}
    where the second line is the specification
    With $f(n)$ being $\operatorname{add}(m)(n)$, we an define it by replacing it with $x$, that is we get $\operatorname{add-succ}_\mathbb{N}(m)(n)(x)\equiv\operatorname{succ}(x)$, as needed. The reason we can replace is follows from the computation rule (as the second parameter of $p_S$ gets ride of $\operatorname{succ}$).
\end{remark}


\begin{definition}{\textbf{(Fibonacci Function)}}
    It is a function $F:\mathbb{N}\rightarrow\mathbb{N}$ that is defined as:
    \begin{equation*}
    \begin{aligned} 
        F(0_\mathbb{N})&:=0_\mathbb{N} \\ 
        F(1_\mathbb{N})&:=1_\mathbb{N} \\ 
        F(\succnat(\succnat(n))) &:= F(\succnat(n)) + F(n) 
    \end{aligned}
    \end{equation*}
    Since $F(\succnat(\succnat(n)))$ is defined using both $F(\succnat(n))$ and $F(n)$, it isn't immediately clear how to present $F$ by the usual induction principle of $\mathbb{N}$. First we note that via the computational rule, we can unwrap the inductive function as:
    \begin{equation*}
    \begin{aligned}
        \operatorname{ind}_\mathbb{N}(p_0,p_S, \succnat( \succnat(n))) &\equiv p_S( \succnat(n), \operatorname{ind}_\mathbb{N}(p_0, p_S,  \succnat(n))) \\
        &\equiv p_S( \succnat(n), p_S(n, \operatorname{ind}_\mathbb{N}(p_0, p_S, n))) \\
    \end{aligned}
    \end{equation*}
    
    Usually we define $F\equiv\operatorname{ind}_\mathbb{N}(p_0, p_S):\mathbb{N}\to \mathbb{N}$, in which the equation above is translated to the equation below:

    \begin{equation*}
    \begin{aligned}
        F(\succnat( \succnat(n))) &\equiv p_S( \succnat(n), F(\succnat(n))) \\
        &\equiv p_S( \succnat(n), p_S(n, F(n))) 
    \end{aligned}
    \end{equation*}
    If we let $p_S(a, b)=b+F(\operatorname{prec}_\mathbb{N}(a))$, where $\operatorname{prec}_\mathbb{N}(x)$ is defined as $\operatorname{prec}_\mathbb{N}(1):=0$ and $\operatorname{prec}_\mathbb{N}(\succnat(x)):=x$
    \begin{equation*}
    \begin{aligned}
        &F(n+2) = p_S(n+1, F(n+1)) = F(n+1) + F(n) \\
        &F(n+2) =p_S(n+1, p_S(n, F(n))) = p_S(n, F(n)) + F(n) = F(n) + F(n-1) + F(n)
    \end{aligned}
    \end{equation*}
\end{definition}

\begin{remark}{(Max Function)}
    We can define the max/min function as follows $\max_\mathbb{N}, \min_\mathbb{N}:\mathbb{N}\to (\mathbb{N}\to \mathbb{N})$
    \begin{equation*}
    \begin{aligned}
        \operatorname{max}_\mathbb{N}(0, b) &\equiv b \\
        \operatorname{max}_\mathbb{N}(a, 0) &\equiv a \\
        \operatorname{max}_\mathbb{N}(a+1, b+1) &\equiv 1+\operatorname{max}_\mathbb{N}(a, b) \\
    \end{aligned} \qquad \quad
    \begin{aligned}
        \operatorname{min}_\mathbb{N}(0, b) &\equiv 0 \\
        \operatorname{min}_\mathbb{N}(a, 0) &\equiv 0 \\
        \operatorname{min}_\mathbb{N}(a+1, b+1) &\equiv 1+\operatorname{min}_\mathbb{N}(a, b) \\
    \end{aligned}
    \end{equation*}
    To implement this, we would have to see the tuple $(a, b)$ as one number, in which the zero value is when either $a$ or $b$ is zero, while the $\operatorname{succ}$ is defined to be $(a,b)\mapsto(a+1,b+1)$.
\end{remark}


\subsection{More Inductive Types}

\begin{remark}{(General Pattern)}
    Just like the type of natural number, other inductive types are also specified by their constructors, and induction principles and their computation rules, which can be described as follows:
    \begin{itemize}
        \item \textbf{(Constructors):} tell what structure the inductive type comes equipped with. These may be any finite number of constructors, event no constructors at all.
        \item \textbf{(Induction Principle):} specifies the data that should be provided in order to construct a section of an arbitrary type family over the induction type.
        \item \textbf{(Computation Rules):} asserts that inductively defined section agrees on the constructors with the data that was used to define the section.
    \end{itemize}
    Since any inductively defined function is entirely determined by its behavior on the constructors, we can again present such inductive definition by pattern matching.
\end{remark}

\begin{definition}{\textbf{(Unit Type)}}
    A type $1$ equipped with a term: $*:\boldsymbol{1}$ satisfying the induction principle that for any family of types $P(x)$ indexed by $x:\boldsymbol{1}$ that is a function as: 
    \begin{equation*}
        \operatorname{ind}_1:P(*)\rightarrow\Pi_{(x:\boldsymbol{1})}P(x)
    \end{equation*}
    for which the computation rule is $\operatorname{ind} _1(p,*)\equiv p$ holds. Alternatively, a definition $f:\Pi_{(x:\boldsymbol{1})}P(x)$ by induction using $p:P(x)$ can be presented by pattern matching $f(*):=p$:
\end{definition}

\begin{remark}{(Special Case of Unit Type)}
    Arises when $P$ doesn't actually depend on $\boldsymbol{1}$. Given a type $A$, then we can weaken it to obtain the constant family over $\boldsymbol{1}$ with $A$. Then the induction principle is a function of $\operatorname{ind}_{\boldsymbol{1}}:A\rightarrow(1\rightarrow A)$. That is for every $x:A$, we have a function $\operatorname{pt}_x:=\operatorname{ind}_1(x):\boldsymbol{1}\rightarrow A$. Given to be:
    \begin{equation*}
        \cfrac{\cfrac{\Gamma\vdash \boldsymbol{1}\type\qquad\Gamma\vdash A\type}{\Gamma, *:\boldsymbol{1}\vdash A\type}\ W}{\Gamma\vdash \operatorname{ind}_1: A\to1\to A\type}
    \end{equation*}
\end{remark}

The empty type is a degenerative example of an inductive type. It doesn't come equipped with any constructors and therefore, there are also no computation rules. Asserting that any type family has a section i.e we can prove anything.

\begin{definition}{\textbf{(Empty Type)}}
    A type $\emptyset$ satisfying to inductive principle that for any family of type $P(x)$ indexed by $x:\emptyset$, there is a term:
    \begin{equation*}
        \operatorname{ind}_\emptyset:\Pi_{(x:\emptyset)}P(x)
    \end{equation*}
\end{definition}

\begin{remark}{(Special Case of Empty Type)}
    In the special case like above, we have a function $\operatorname{ex-falso}:=\operatorname{ind}_\emptyset:\emptyset\rightarrow A$ for any type $A$. To obtained this, we weaken $A$ to get constant family over $\emptyset$ with value $A$, and then the function follows from the induction principle.
\end{remark}

\begin{definition}{\textbf{(Negation/Empty)}}
    For any type $A$, we define negation of $A$ by: $\neg A := A\rightarrow\emptyset$. Type $A$ is empty if it comes equipped with an element of type $\neg A$ i.e $\operatorname{is-empty}(A)\equiv A\to \emptyset$
\end{definition}

\begin{definition}{\textbf{(Proof of Negation)}}
    The proof of  $\neg A$ is given by assuming that $A$ holds and then constructing an element of the empty type. That is we prove $\neg A$ by assuming $A$ and deriving a contradiction. 
\end{definition}

\begin{remark}{\textit{(Proof of Contradiction vs Negation)}}
    This is different from the proof by contradiction because for proof by contradiction of a property $P$ is an argument where we conclude that $P$ holds after showing that $\neg P$ implies a contradiction i.e the use of double negation elimination $\neg\neg P\implies P$.

    In type theory, the type $\neg\neg A$ is type of function as $(A\rightarrow\emptyset)\rightarrow\emptyset$. This is different from $A$ itself as it isn't possible to construct $\neg\neg A\rightarrow A$ unless we know more about $A$.
\end{remark}

\begin{proposition}
    For any 2 types $P$ and $Q$, there is a function:
    \begin{equation*}
        (P\rightarrow Q)\rightarrow (\neg Q\rightarrow \neg P)
    \end{equation*}
\end{proposition}
\begin{proof}
    We will define the desired function as $\lambda$-abstraction. Assuming we have a function $f:P\to Q$, $\tilde{q}:\neg Q:=Q\to\emptyset$ and $p:P$, then we have: $\lambda f.\lambda\tilde{q}.\lambda p.\tilde{q}(f(p)) : (P\rightarrow Q)\rightarrow (\neg Q\rightarrow \neg P)$. The natural deduction tree is similar to how we define the composition function.
\end{proof}


\begin{definition}{\textbf{(Co-Product)}}
    Let $A$ and $B$ be types, the co-product $A+B$ is a type that comes equipped with: $\operatorname{inl}:A\rightarrow A+B$ and $\operatorname{inr}:B\rightarrow A+B$. Satisfying the induction principle that for any family of types $P(x)$ index by $x:A+B$, there is a term (in the first line):
    \begin{equation*}
    \begin{aligned}
        &\operatorname{ind}_+ : \Big( \Pi_{(x:A)}P(\operatorname{inl}(x)) \Big) \rightarrow \Big( \Pi_{(y:B)}P(\operatorname{inr}(y)) \Big) \rightarrow \Pi_{(z:A+B)}P(z) \\
        &\operatorname{ind}_+ \big( f,g,\operatorname{inl}(x) \big) \equiv f(x) \qquad \operatorname{ind}_+ \big( f,g,\operatorname{inr}(x) \big)\equiv g(x)
    \end{aligned}
    \end{equation*}
    for which the computation rule can be given by, the second line. 
    
    \textbf{(Pattern Matching):} Dependency function $h:\Pi_{(x:A+B)}P(x)$ defined by induction using $f:\Pi_{(x:A)}P(\operatorname{inl}(x))$ and $g:\Pi_{(y:B)}P(\operatorname{inr}(y))$ can be presented by pattern matching as:
    \begin{equation*}
        h(\operatorname{inl}(x)) := f(x) \qquad \quad h(\operatorname{inr}(y)):=g(y)
    \end{equation*}
    Sometimes, we write $[f,g]$ for the function $\operatorname{ind} _+(f,g)$. The co-product of 2 types is sometimes also called the disjoint sum. 
\end{definition}

\begin{remark}{(Special Case for Co-Product)}
    By induction principle of co-product, we obtain a function:
    \begin{equation*}
        \operatorname{ind}_+:(A\rightarrow X)\rightarrow\big((B\rightarrow X)\rightarrow (A+B\rightarrow X)\big)
    \end{equation*}
    for any type $X$. This is very similar to the elimination rule of disjunction in the first order logic: $A,P,P'$ and $Q$ are proposition, which: $(P\implies Q) \implies \big( (P'\implies Q)\implies (P\vee P')\implies Q \big)$. Under this interpretation of type theory the co-product is indeed the disjunction.
\end{remark}

\begin{remark}
    The induction principle for co-products gives us the map: $f+g : A+B\rightarrow A'+B'$ for every $f:A\rightarrow A'$ and $g:B\rightarrow B'$. Indeed, the map $f+g$ is defined to be:
    \begin{equation*}
    \begin{aligned}
        (f+g)\big(\operatorname{inl}(x)\big) := \operatorname{inl}(f(x)) \qquad \quad (f+g)\big(\operatorname{inr}(x)\big) := \operatorname{inl}(g(y))
    \end{aligned}
    \end{equation*}
    where we have $f+g:=\operatorname{ind}_+(\operatorname{inl}_{A'+B'}\circ f, \operatorname{inr}_{A'+B'}\circ g)$ note that we have the following signature: $\operatorname{inl}\circ f:A\to A'\to A'+B'$ and $\operatorname{inr}\circ f:B\to B'\to A'+B'$.
\end{remark}

\begin{proposition}
    Consider 2 types $A$ and $B\equiv\emptyset$. Then there is a function $(A+B)\rightarrow A$
\end{proposition}
\begin{proof}
    Since $B$ is empty, we have that $\tilde{b}:B\to\emptyset$ together with the function $\operatorname{ex-falso}:\emptyset\to A$ note that we have $\operatorname{ex-false}\circ\tilde{b}:B\to A$. On the other hand, we have the identity function $\operatorname{id}_A:A\to A$. Thus, we can construct the function $(A+B)\to A$ with the inductive properties of the co-product $A+B$ we have the function $\operatorname{ind}_+(\operatorname{id}_A,\operatorname{ex-false}\circ\tilde{b}):A+B\to B$, as needed.
\end{proof}

\begin{definition}{\textbf{(Booleans)}}
    Inductive type $\operatorname{bool}$ that comes equipped with $\operatorname{false}:\operatorname{bool}$ and $\operatorname{true}:\operatorname{bool}$. The induction principle of the booleans asserts that for any family of types $P(x)$ indexed by $x:\operatorname{bool}$ there is a term:
    \begin{equation*}
        \operatorname{ind-bool}: P(\operatorname{false}) \to \left( P(\operatorname{true})\to\Pi_{x:\operatorname{bool}}P(x) \right)
    \end{equation*}
    With the computation rules of: $\operatorname{ind-bool}(p_0,p_1,\operatorname{false}):=p_0$ and $\operatorname{ind-bool}(p_0,p_1,\operatorname{true}):=p_1$
\end{definition}

\begin{proposition}
    We can construct the following functions: $\operatorname{neg-bool}:\operatorname{bool}\to\operatorname{bool}$, the boolean conjunction operation $-\wedge-:\operatorname{bool}\to(\operatorname{bool}\to\operatorname{bool})$ and the boolean disjunction operation $-\vee-:\operatorname{bool}\to(\operatorname{bool}\to\operatorname{bool})$
\end{proposition}
\begin{proof}
    We can define for eacgh of the following functions: (1) Negation $\operatorname{neg-bool}\equiv\operatorname{ind-bool}(\operatorname{true}, \operatorname{false})$. (2) conjunction $\lambda a.\lambda b.\operatorname{ind-bool}(\operatorname{false}, a, b)$ (3) disjunction $\lambda a.\lambda b.\operatorname{ind-bool}(a,\operatorname{true},b)$
\end{proof}

\begin{remark}{(Notes on Constructing Integer)}
    The set of integers is usually defined as a quotient of the set $\mathbb{N}\times\mathbb{N}$, where $\big( (n,m)\sim(n',m') \big) := \big(n+m'=n'+m\big)$. The identity type haven't been introduced yet. Furthermore, there are no quotient types in Martin-Lof's dependent type theory (much later on that we will see it). 
\end{remark}

\begin{definition}{\textbf{(Integer)}}
    The integer is a type $\mathbb{Z}:\equiv \mathbb{N}+(1+\mathbb{N})$, with the inclusion function of the positive and negative integer: $\operatorname{in-pos} :\equiv \operatorname{inr}\circ\operatorname{inr}:\mathbb{N}\rightarrow\mathbb{Z}$ and $\operatorname{in-neg} :\equiv \operatorname{inl}:\mathbb{N}\rightarrow\mathbb{Z}$ and the constants can be defined as:
    \begin{equation*}
    \begin{aligned}
        -1_\mathbb{Z} :\equiv \operatorname{in-neg}(0) \qquad\quad
        0_\mathbb{Z} :\equiv \operatorname{inr}(\operatorname{inl}(*)) \qquad\quad
        1_\mathbb{Z} :\equiv \operatorname{in-pos}(0)
    \end{aligned}
    \end{equation*}
    It is possible to derive an inductive principle, which asserts that for any type  family $P$ over $\mathbb{Z}$. We have the dependent function $f:\Pi_{(k:\mathbb{Z})}P(k)$ in which:
    \begin{equation*}
    \begin{aligned} 
        f(-1_\mathbb{Z}) &\equiv p_{-1} \\
        f(\operatorname{in-neg}(\succnat(n))) &\equiv p_{-S}(n, f(\operatorname{in-neg}(n))) \\ f(0_\mathbb{Z}) &\equiv p_{0} \\ 
        f(1_\mathbb{Z}) &\equiv p_{1} \\ 
        f(\operatorname{in-pos}(\succnat(n))) &\equiv p_{S}(n, f(\operatorname{in-pos}(n))) \\ 
    \end{aligned} \qquad \quad
    \begin{aligned} 
        p_{-1}&:P(-1_\mathbb{Z}) \\ 
        p_{-S}&: \Pi_{(n:\mathbb{N})}P(\operatorname{in-neg}(n))\rightarrow P(\operatorname{in-neg}(\succnat(n))) \\ 
        p_{0}&:P(0_\mathbb{Z}) \\ p_{1}&:P(1_\mathbb{Z}) \\ 
        p_{S}&: \Pi_{(n:\mathbb{N})}P(\operatorname{in-pos}(n))\rightarrow P(\operatorname{in-pos}(\succnat(n))) \\ 
    \end{aligned}
    \end{equation*}
    Note that, semantically, we have $\operatorname{in-neg}(n)\equiv -n-1$ and $\operatorname{in-pos}(n)\equiv n+1$
\end{definition}

We have the following successor function to be defined as:

\begin{definition}{\textbf{(Successor/Predecessor Function on Integer)}}
    \label{def:succ-pred-int}
    Both $\operatorname{succ}_\mathbb{Z}:\mathbb{Z}\rightarrow\mathbb{Z}$ and $\operatorname{pred}_\mathbb{Z}:\mathbb{Z}\rightarrow\mathbb{Z}$ can be defined inductively as
    \begin{equation*}
    \begin{aligned} 
        \operatorname{succ}_\mathbb{Z}(-1_\mathbb{Z}) &\equiv0_\mathbb{Z} \\ 
        \operatorname{succ}_\mathbb{Z}(0_\mathbb{Z}) &\equiv 1_\mathbb{Z} \\ 
        \operatorname{succ}_\mathbb{Z}(1_\mathbb{Z}) &\equiv \operatorname{in-pos}(1_\mathbb{N}) \\ 
        \operatorname{succ}_\mathbb{Z}(\operatorname{in-neg}(\succnat(n))) &\equiv\operatorname{in-neg}(n) \\ 
        \operatorname{succ}_\mathbb{Z}(\operatorname{in-pos}(\succnat(n))) &\equiv\operatorname{in-pos}(\succnat(\succnat(n))) \\ 
    \end{aligned} \qquad \quad 
    \begin{aligned} 
        \operatorname{pred}_\mathbb{Z}(-1_\mathbb{Z}) &\equiv \operatorname{in-neg}(1_\mathbb{N}) \\ 
        \operatorname{pred}_\mathbb{Z}(0_\mathbb{Z}) &\equiv -1_\mathbb{Z} \\ 
        \operatorname{pred}_\mathbb{Z}(1_\mathbb{Z}) &\equiv 0_\mathbb{Z} \\ 
        \operatorname{pred}_\mathbb{Z}(\operatorname{in-neg}(\succnat(n))) &\equiv\operatorname{in-neg}(\succnat(\succnat(n))) \\ 
        \operatorname{pred}_\mathbb{Z}(\operatorname{in-pos}(\succnat(n))) &\equiv\operatorname{in-pos}(n) \\ 
    \end{aligned}
    \end{equation*}
    For the case of $\operatorname{succ}_\mathbb{Z}(\operatorname{in-neg}(\succnat(n)))$, so we have $\operatorname{in-neg}(n+1)\equiv -n-1-1$ and so $\operatorname{succ}_\mathbb{Z}(-n-2)\equiv -n-1$.
\end{definition}

\begin{definition}{\textbf{(Addition)}}
    We can define $\operatorname{add}_\mathbb{Z}:\mathbb{Z}\to(\mathbb{Z}\to \mathbb{Z})$ in similar manners to the natural number case that is we define $\operatorname{add}_\mathbb{Z}(m)$ (this is added by $m$) and $\operatorname{add}_\mathbb{Z}(\operatorname{succ}_\mathbb{Z}(\operatorname{in-neg}(m)))$ (this is added by $-m$)
    \begin{equation*}
    \begin{aligned}[t]
        \operatorname{add}_\mathbb{Z}(m)(-1_\mathbb{Z}) &\equiv\operatorname{pred}_\mathbb{Z}(m) \\ 
        \operatorname{add}_\mathbb{Z}(m)(0_\mathbb{Z}) &\equiv m \\ 
        \operatorname{add}_\mathbb{Z}(m)(1_\mathbb{Z}) &\equiv \succnat(m) \\ 
    \end{aligned}
    \qquad \qquad
    \begin{aligned}[t]
        \operatorname{add}_\mathbb{Z}(m)(\operatorname{in-neg}(\succnat(n))) &\equiv\operatorname{add}_\mathbb{Z}(\operatorname{prec}_\mathbb{Z}(m))(\operatorname{in-neg}(n)) \\ 
        \operatorname{add}_\mathbb{Z}(m)(\operatorname{in-pos}(\succnat(n))) &\equiv\operatorname{add}_\mathbb{Z}(\operatorname{succ}_\mathbb{Z}(m))(\operatorname{in-pos}(n)) \\ 
    \end{aligned}
    \end{equation*}
    For the first case of the second column, we have $(m-1)+(-n-1)=m-n-2$, similar for the second case, we have that $m+n+2=(m+1)+(n+1)$
\end{definition}

\begin{definition}{\textbf{(Inverse)}}
    We can define $\operatorname{neg}_\mathbb{Z}:\mathbb{Z}\to \mathbb{Z}$ as:
    \begin{equation*}
    \begin{aligned}[t]
        \operatorname{neg}_\mathbb{Z}(-1_\mathbb{Z}) &\equiv1_\mathbb{Z} \\ 
        \operatorname{neg}_\mathbb{Z}(0_\mathbb{Z}) &\equiv 0_\mathbb{Z} \\ 
        \operatorname{neg}_\mathbb{Z}(1_\mathbb{Z}) &\equiv -1_\mathbb{Z} \\ 
    \end{aligned} \qquad \quad 
    \begin{aligned}[t]
        \operatorname{neg}_\mathbb{Z}(\operatorname{in-neg}(\succnat(n))) &\equiv\operatorname{in-pos}(\succnat(n)) \\ 
        \operatorname{neg}_\mathbb{Z}(\operatorname{in-pos}(\succnat(n))) &\equiv\operatorname{in-neg}(\succnat(n)) \\ 
    \end{aligned}
    \end{equation*}
\end{definition}

Now, we have defined the group operation for the integer. 

\begin{definition}{\textbf{(Dependent Pair Type)}}
    Given a type family $B$ over $A$, we can define the dependent pair type (or $\Sigma$-Type) to be inductive type $\Sigma_{(x:A)}B(x)$ equipped with a pairing function: $\operatorname{pair}: \Pi_{(x:A)}\big(B(x)\rightarrow \Sigma_{(y:A)}B(y)\big)$. The induction principle $\Sigma_{(x:A)}B(x)$ asserts that for any family of types $P(p)$ indexed by $p:\Sigma_{(x:A)}B(x)$, we have:
    \begin{equation*}
        \operatorname{ind}_\Sigma : \Big(\Pi_{(x:A)}\Pi_{(y:B(x))}P(\operatorname{pair}(x, y))\Big) \rightarrow \Big(\Pi_{(z:\Sigma_{(x:A)}B(x))}P(z)\Big)
    \end{equation*}
    satisfy the following computation rules of $\operatorname{ind}_\Sigma(g, \operatorname{pair}(x,y))\equiv g(x, y)$, where $g:\prod_{x:A}B(x)\to P(\operatorname{pair}(x, y))$ and note that $\operatorname{pair}(x, y)\equiv\Sigma_{x:A}B(x)$. Alternatively, a definition of a dependent function $f:\Pi_{(z:\Sigma_{(x:A)}B(x))}P(z)$ by induction using a function $g:\Pi_{(x:A)}\Pi_{(y:B(x))}P((x,y))$ can be presented as $f(\operatorname{pair}(x,y)) \equiv g(x,y)$
\end{definition}

The induction principle of $\Sigma$-type can be used to define the projection function:
\begin{definition}{\textbf{(Projection)}}
    Consider a type $A$ and a type family $B$ over $A$, then we have: (1) The first projection map: $\operatorname{pr}_1:(\Sigma_{(x:A)}B(x))\rightarrow A$ is defined by induction as $\operatorname{pr}_1(x,y):\equiv x$. (2) The second projection map: $\operatorname{pr}_2:\Pi_{(p:\Sigma_{(x:A)}B(x))}B(\operatorname{pr}_1(p))$ is defined by induction as: $\operatorname{pr}_2:(x,y)\equiv y$.
\end{definition}

\begin{remark}{(More Details on Projection Map)}
    For the first projection map, define the map $\Pi_{x:A}B(x)\to A$ (the family of types in this case is $A$) to return only the first element $A$ i.e $f_1:=\lambda.a\lambda.b.a$, then we set $\operatorname{pr}_1\equiv\operatorname{ind}_\Sigma(f_1)$. The case for the second projection map is the same.
\end{remark}

\begin{remark}{\textbf{(Currying + Uncurrying)}}
    If we want to construct a function: $f:\Pi_{(z:\Sigma_{(x:A)}B(x))}P(z)$, by $\Sigma$-induction, then we get to assume a pair $(x,y)$ consisting of $x:A$ and $y:B(x)$ and our goal will be to construct an element of type $P(x,y)$. Thus it is an converse of \textit{currying operation}, which given to be:
    \begin{equation*}
        \operatorname{ev-pair} : \Big( \Pi_{(z:\Sigma_{(x:A)}B(x))}P(z) \Big) \rightarrow \Big( \Pi_{(x:A)}\Pi_{y:B(x)}P(x,y) \Big)
    \end{equation*}
    given by $f\mapsto\lambda x.\lambda y.f(x,y)$. Thus it is known as the uncurrying operation.
\end{remark}

Similar to other cases, we can consider a special case of $\Sigma$-type, which happens when $B$ is constant family over $A$. Thus, $\Sigma_{(x:A)}B$ is a type of an ordinary pair $(x,y)$ where $x:A$ and $y:B$.

\begin{definition}{\textbf{(Cartesian Product)}}
    Given $2$ types $A$ and $B$. Then we define the cartesian product $A\times B$ of $A$ and $B$ by: $A\times B := \Sigma_{(x:A)}B$. It satisfies the induction principle of $\Sigma$-types, in which for any type family $P$ over $A\times B$ there is a function:
    \begin{equation*}
        \operatorname{ind}_\times:\Big( \Pi_{(x:A)}\Pi_{(y:B)}P(x,y) \Big)\rightarrow\Big( \Pi_{(z:A\times B)}P(z) \Big)
    \end{equation*}
    Satisfies the computation rule as: $\operatorname{ind}_\times(g,(x,y))\equiv g(x,y)$. The projection mapping is defined in similar way to $\Sigma$-type, above. Finally, when think of types as proposition, $A\times B$ is interpreted as conjunction of $A$ and $B$.
\end{definition}

\begin{proposition}
    $\neg(P\times \neg P)$ and $\neg(P\leftrightarrow\neg P)$
\end{proposition}
\begin{proof}
    We will consider the use of proof of negation, in which we will construct element of $\emptyset$ from the inner statement.
    \textbf{(Part 1):} Given the terms $a:P\times \neg P$, we can see that $\operatorname{proj}_2(a)(\operatorname{proj}_1(a)):\emptyset$. Note that $a\equiv(p:P, f:P\to\emptyset)$.

    {\color{violet} \textbf{(Part 2):} Note that} $P\leftrightarrow\neg P:\equiv(P\to\neg P)\times(\neg P\to P)$, given a term $a:P\leftrightarrow\neg P$, then $(p_1,p_2)\equiv a$. Then we have $p_1:P\to(P\to\emptyset)$ and $p_2:(P\to\emptyset)\to P$. We can define the function $f\equiv\lambda x.p_2(x, \cdot):P\to P$, in which we can apply $p_1(f):\emptyset$. Thus, we have created the term of empty type.
\end{proof}

{\color{violet} \begin{proposition}
    We have the double negation monad, in which: $P\to\neg\neg P$ and $(P\to Q)\to(\neg\neg P\to\neg\neg Q)$ and $(P\to\neg\neg Q)\to(\neg\neg P\to\neg\neg Q)$
\end{proposition}}
\begin{proof}

    For each of them we can make the following construction:
    \begin{itemize}
        \item We can construct $P\to((P\to\emptyset)\to\emptyset)$ by setting it to be $\lambda x.\lambda f.f(x)$. 
        \item We can construct $\big(P\to Q\big)\to\big( ((P\to\emptyset)\to\emptyset) \to ((Q\to\emptyset)\to\emptyset) \big)$. Suppose we have $f:P\to Q, g:(P\to\emptyset)\to\emptyset$ and $h:Q\to\emptyset$. Then we can set $\lambda f.\lambda g.\lambda h.g((h\circ f))$.
        \item We have that: $\big(P\to((Q\to\emptyset)\to\emptyset)\big)\to\big(((P\to\emptyset)\to\emptyset)\to((Q\to\emptyset)\to\emptyset)\big)$. Suppose that we have $f:(Q\to\emptyset)\to\emptyset$ with $g:(P\to\emptyset)\to\emptyset$ and $h:Q\to\emptyset$ with $p:P$. We have: $\lambda p.\lambda f.\lambda g.\lambda h. g(p, f(h))$.
    \end{itemize}
\end{proof}

\begin{definition}{\textbf{(List Type)}}
    For any type $A$, we can define a type $\operatorname{list}(A)$ of lists of elements of $A$ as inductive type with constructors of $\operatorname{nil}:\operatorname{list}(A)$ and $\operatorname{cons}:A\to(\operatorname{list}(A)\to\operatorname{list}(A))$. Then the induction principle is:
    \begin{equation*}
        \operatorname{ind}_{\operatorname{list}(A)}: P(\operatorname{nil})\to\Big(\Pi_{a:A}\Pi_{l:P(\operatorname{list}(A))}P(\operatorname{con}(a, l))\Big)\to\big(\Pi_{l:\operatorname{list}(A)}P(l)\big)
    \end{equation*}
    The computation rule is that if we want to define $f:\Pi_{l:\operatorname{list}(A)}P(l)$, we can define $f(\operatorname{nil})\equiv p_{\operatorname{nil}}$ where $p_{\operatorname{nil}}:P(\operatorname{Nil})$ and {\color{violet}$f(\operatorname{cons}(a, l))\equiv g(a, f(l))$ where $g:\Pi_{a:A}\Pi_{l:P(\operatorname{list}(A))}P(\operatorname{con}(a, l))$}
\end{definition}

\begin{definition}{\textbf{(Fold/Map/Length/Reverse)}}
    These are the usual function on the list type: given $A$ and $B$ be types:
    \begin{itemize}
        \item (Fold): Suppose that $b:B$ and consider binary operation $\mu:A\to(B\to B)$. We define a function $\operatorname{fold-list}(\mu):\operatorname{list}(A)\to B$ as (following from the induction principle):
        \begin{equation*}
            \operatorname{fold-list}(\mu, \operatorname{nil}):=b\qquad \quad \operatorname{fold-list}(\mu, \operatorname{cons}(a, l)):=\mu(a, \operatorname{fold-list}(\mu, l))
        \end{equation*}
        \item (Map): We can also define the operation: $\operatorname{map-list}:(A\to B)\to(\operatorname{list}(A)\to\operatorname{list}(B))$ in which (following from the induction principle):
        \begin{equation*}
            \operatorname{map-list}(f, \operatorname{nil}):=\operatorname{nil}\qquad\quad \operatorname{map-list}(f,\operatorname{cons}(a, l)):= \operatorname{cons}(f(a), \operatorname{map-list}(f, l)) 
        \end{equation*}
        \item (Length): Finally, we have $\operatorname{length-list}:\operatorname{list}(A)\to \mathbb{N}$, where:
        \begin{equation*}
            \operatorname{length-list}(\operatorname{nil}) := 0 \qquad \quad \operatorname{length-list}(\operatorname{cons}(a, l)) := 1 + \operatorname{length-list}(l)
        \end{equation*}
        \item (Reverse): $\operatorname{revese}:=\operatorname{fold-list}(\operatorname{cons})$, note that that $\operatorname{cons}$ is append at the back with the base case of $\operatorname{nil}$. That is we have the following sequence:
        \begin{equation*}
        \begin{aligned}
            \operatorname{reverse}([1, 2, 3])&=\operatorname{cons}(1, \operatorname{cons}(2, \operatorname{cons}(3, [\cdot])))\\
            &= \operatorname{cons}(1, \operatorname{cons}(2, [3]))=\operatorname{cons}(1, [3, 2])=[3,2,1]
        \end{aligned}
        \end{equation*}
    \end{itemize}
\end{definition}


